{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbMBlyAPZidR"
   },
   "source": [
    "# Language Model training & model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PMTbT4hY-ZWr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "import math\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import torchtext\n",
    "import copy\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9TPHFhiZidV"
   },
   "source": [
    "### Load train, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ehm9ht4dZidV"
   },
   "outputs": [],
   "source": [
    "data_folder = 'datasets/LM-training-datasets'\n",
    "train_df = pd.read_csv(os.path.join(data_folder, 'train.csv'))\n",
    "validation_df = pd.read_csv(os.path.join(data_folder, 'validation.csv'))\n",
    "test_df = pd.read_csv(os.path.join(data_folder, 'test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKOmCpI2ZidW"
   },
   "source": [
    "### Setting global features and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "r8y3Zs28ZidW"
   },
   "outputs": [],
   "source": [
    "# Global features\n",
    "n_epochs = 200\n",
    "MAX_DEPTHS = [5,10] # max depth of each path\n",
    "MIN_FREQS = [3,5] # minimum frequency of each word in the vocabulary\n",
    "\n",
    "# Hyperparameters to test(each variable is a list of possible values)\n",
    "embedding_size = [128, 256,512]\n",
    "n_layers = [2,3,4]\n",
    "dropout_rate = [0.2,0.4,0.6]\n",
    "\n",
    "# Combinations of hyperparameters\n",
    "hyperparameters = [(es, nl, dr) for es in embedding_size for nl in n_layers for dr in dropout_rate]\n",
    "\n",
    "# Other hyperparameters (not to be tested)\n",
    "batch_size = 128\n",
    "lr=1e-3\n",
    "tie_weights = True # if True, helps to reduce the number of parameters\n",
    "                   # if True, hidden_dim = embedding_dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4QA3Cs7ZidW"
   },
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ff5kj-rxZidW"
   },
   "outputs": [],
   "source": [
    "# Custom tokenizer to prepare the data\n",
    "\n",
    "year_token='YEAR'\n",
    "\n",
    "def custom_tokenizer(path,MAX_DEPTH):\n",
    "\n",
    "    # remove leading slash\n",
    "    path=path.lstrip('/')\n",
    "\n",
    "    # Split the path into words\n",
    "    path_words = path.split('/')\n",
    "\n",
    "    # Trim the path to MAX_DEPTH\n",
    "    path_words = path_words[:MAX_DEPTH]\n",
    "\n",
    "    #YEAR token substitution for 4-digit numbers\n",
    "    for i,tok in enumerate(path_words):\n",
    "        pattern = r'^\\d{4}$'\n",
    "        if re.match(pattern, tok):\n",
    "            path_words[i]=year_token\n",
    "\n",
    "    return path_words\n",
    "\n",
    "# Function to yield tokens from the DataFrame\n",
    "def yield_tokens(data_iter,MAX_DEPTH):\n",
    "    for path in data_iter:\n",
    "        yield custom_tokenizer(path, MAX_DEPTH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jqY91z2oZidW"
   },
   "outputs": [],
   "source": [
    "# Create the vocabulary\n",
    "def create_vocabulary(MIN_FREQ,MAX_DEPTH):\n",
    "    vocab = torchtext.vocab.build_vocab_from_iterator(yield_tokens(train_df['Path'],MAX_DEPTH), min_freq=MIN_FREQ)\n",
    "    vocab.insert_token('<unk>', 0)\n",
    "    vocab.insert_token('<eos>', 2)\n",
    "    vocab.insert_token('<sos>', 1)\n",
    "    vocab.insert_token('<pad>',3)\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "    # print(f'len vocab = {len(vocab)}')\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DH--icKGZidX"
   },
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "def get_dataloader(tokens, vocab, batch_size, seq_len):\n",
    "    data = []\n",
    "    for token_list in tokens:\n",
    "        token_list.append('<eos>')\n",
    "        token_list=['<sos>']+token_list\n",
    "        while len(token_list)<seq_len:\n",
    "            token_list.append('<pad>')\n",
    "        mapped_tokens= [vocab[token] for token in token_list]\n",
    "        data.extend(mapped_tokens)\n",
    "    data = torch.LongTensor(data)\n",
    "    num_batches = data.shape[0] // batch_size\n",
    "    data = data[:num_batches * batch_size]\n",
    "    data = data.view(batch_size, num_batches)\n",
    "    return data\n",
    "\n",
    "def get_dataloaders(MIN_FREQ,MAX_DEPTH,seq_len):\n",
    "  vocab=create_vocabulary(MIN_FREQ,MAX_DEPTH)\n",
    "  train_data = get_dataloader([custom_tokenizer(url, MAX_DEPTH) for url in train_df['Path']], vocab, batch_size, seq_len)\n",
    "  valid_data = get_dataloader([custom_tokenizer(url , MAX_DEPTH) for url in validation_df['Path']], vocab, batch_size, seq_len)\n",
    "  return train_data,valid_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSl0y58eZidX"
   },
   "source": [
    "### Language Model definition, training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_7Vea0YQZidX"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate,\n",
    "                tie_weights):\n",
    "\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.tie_weights = tie_weights\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # Dropout between embedding and lstm\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n",
    "                    dropout=dropout_rate, batch_first=True)\n",
    "\n",
    "        # Dropout between lstm and fc\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        if tie_weights:\n",
    "            assert embedding_dim == hidden_dim, 'cannot tie, check dims'\n",
    "            self.embedding.weight = self.fc.weight\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, src, hidden):\n",
    "        embedding = self.dropout(self.embedding(src))\n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "        output = self.dropout(output)\n",
    "        prediction = self.fc(output)\n",
    "        return prediction, hidden\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_range_emb = 0.1\n",
    "        init_range_other = 1/math.sqrt(self.hidden_dim)\n",
    "        self.embedding.weight.data.uniform_(-init_range_emb, init_range_emb)\n",
    "        self.fc.weight.data.uniform_(-init_range_other, init_range_other)\n",
    "        self.fc.bias.data.zero_()\n",
    "        for i in range(self.num_layers):\n",
    "            self.lstm.all_weights[i][0] = torch.FloatTensor(self.embedding_dim,\n",
    "                    self.hidden_dim).uniform_(-init_range_other, init_range_other)\n",
    "            self.lstm.all_weights[i][1] = torch.FloatTensor(self.hidden_dim,\n",
    "                    self.hidden_dim).uniform_(-init_range_other, init_range_other)\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "    def detach_hidden(self, hidden):\n",
    "        hidden, cell = hidden\n",
    "        hidden = hidden.detach()\n",
    "        cell = cell.detach()\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gM_URGeMZidY"
   },
   "outputs": [],
   "source": [
    "# get_batch and train (one epoch) functions\n",
    "def get_batch(data, seq_len, num_batches, idx):\n",
    "    src = data[:, idx:idx+seq_len]\n",
    "    target = data[:, idx+1:idx+seq_len+1]\n",
    "    return src, target\n",
    "\n",
    "def get_batch(data, seq_len, num_batches, idx):\n",
    "    src = data[:, idx:idx+seq_len]\n",
    "    target = data[:, idx+1:idx+seq_len+1]\n",
    "    return src, target\n",
    "\n",
    "\n",
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    for idx in range(0, num_batches - 1, seq_len):  # The last batch can't be a src\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.detach_hidden(hidden)\n",
    "\n",
    "        src, target = get_batch(data, seq_len, num_batches, idx)\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, hidden = model(src, hidden)\n",
    "\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Bp_bN9_SZidY"
   },
   "outputs": [],
   "source": [
    "# Evaluation function (one epoch) (used for early stopping)\n",
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            hidden = model.detach_hidden(hidden)\n",
    "            src, target = get_batch(data, seq_len, num_batches, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            batch_size= src.shape[0]\n",
    "\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUV9rkZpZidY"
   },
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "miLaQKSTZidY"
   },
   "outputs": [],
   "source": [
    "# Training function for one set of hyperparameters\n",
    "\n",
    "def train_model(n_epochs, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights, MIN_FREQ, MAX_DEPTH, device, batch_size, lr):\n",
    "    seq_len=MAX_DEPTH + 2 # MAX DEPTH + sos and eos tokens\n",
    "    vocab = create_vocabulary(MIN_FREQ,MAX_DEPTH)\n",
    "    vocab_size=len(vocab)\n",
    "    train_data, valid_data = get_dataloaders(MIN_FREQ,MAX_DEPTH,seq_len)\n",
    "\n",
    "    model = LSTM(vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, tie_weights).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])\n",
    "\n",
    "    # Used to reduce the learning rate by a factor of 2 after every epoch associated with no improvement in the validation loss\n",
    "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "    clip= 0.25\n",
    "\n",
    "    # Early stopping\n",
    "    best_valid_loss = float('inf')\n",
    "    early_stopping_patience = 10 # Number of epochs to wait before stopping\n",
    "    early_stopping_counter = 0 # Counter for early stopping\n",
    "    best_model_state_dict=copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for epoch in tqdm(range(1, n_epochs+1)):\n",
    "        train_loss = train(model, train_data, optimizer, criterion,\n",
    "                    batch_size, seq_len, clip, device)\n",
    "        valid_loss = evaluate(model, valid_data, criterion, batch_size,\n",
    "                    seq_len, device)\n",
    "\n",
    "        lr_scheduler.step(valid_loss)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_model_state_dict=copy.deepcopy(model.state_dict())\n",
    "            early_stopping_counter = 0 # Reset the counter if the validation loss improves\n",
    "        else:\n",
    "            early_stopping_counter += 1 # Increment the counter if the validation loss does not improve\n",
    "\n",
    "        # print(f'Epoch {epoch}, train_loss = {train_loss:.4f}, valid_loss = {valid_loss:.4f}')\n",
    "\n",
    "        # Check if early stopping condition is met\n",
    "        if early_stopping_counter >= early_stopping_patience:\n",
    "            # print(f\"Early stopping at epoch {epoch} due to no improvement in validation loss for {early_stopping_patience} epochs.\")\n",
    "            break # Stop the training loop\n",
    "\n",
    "    return best_model_state_dict, best_valid_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSmCHZTkZidY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter combination: MAX_DEPT=5, MIN_FREQ=3, (128, 2, 0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 10/200 [5:34:15<105:51:03, 2005.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter combination: MAX_DEPT=5, MIN_FREQ=3, (128, 2, 0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 10/200 [5:23:37<102:28:47, 1941.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter combination: MAX_DEPT=5, MIN_FREQ=3, (128, 2, 0.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 11/200 [5:59:07<102:50:31, 1958.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter combination: MAX_DEPT=5, MIN_FREQ=3, (128, 3, 0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 10/200 [5:46:11<109:37:30, 2077.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter combination: MAX_DEPT=5, MIN_FREQ=3, (128, 3, 0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 10/200 [5:24:34<102:46:48, 1947.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter combination: MAX_DEPT=5, MIN_FREQ=3, (128, 3, 0.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 11/200 [5:48:52<99:54:11, 1902.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter combination: MAX_DEPT=5, MIN_FREQ=3, (128, 4, 0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 10/200 [5:39:12<107:24:52, 2035.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter combination: MAX_DEPT=5, MIN_FREQ=3, (128, 4, 0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 10/200 [5:32:04<105:09:23, 1992.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter combination: MAX_DEPT=5, MIN_FREQ=3, (128, 4, 0.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 11/200 [6:31:14<112:02:11, 2134.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter combination: MAX_DEPT=5, MIN_FREQ=3, (256, 2, 0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Train for every hyperparameter combination\n",
    "results = []\n",
    "best_models = {}\n",
    "global_param_combinations= [(max_depth,min_freq) for max_depth in MAX_DEPTHS for min_freq in MIN_FREQS]\n",
    "\n",
    "\n",
    "for MAX_DEPTH, MIN_FREQ in global_param_combinations:\n",
    "    for hyperparameter in hyperparameters:\n",
    "        print(f'Hyperparameter combination: MAX_DEPT={MAX_DEPTH}, MIN_FREQ={MIN_FREQ}, {hyperparameter}')\n",
    "        embedding_dim, num_layers, dropout_rate = hyperparameter\n",
    "        model_state_dict, valid_loss = train_model(n_epochs, embedding_dim, embedding_dim, num_layers, dropout_rate, tie_weights, MIN_FREQ, MAX_DEPTH, device, batch_size, lr)\n",
    "        results.append((MAX_DEPTH, MIN_FREQ, hyperparameter, valid_loss, model_state_dict))\n",
    "\n",
    "        # Check if this model has the lowest validation loss for the current global parameter combination\n",
    "        if (MAX_DEPTH, MIN_FREQ) not in best_models or valid_loss < best_models[(MAX_DEPTH, MIN_FREQ)][1]:\n",
    "            # Save the model state dictionary and validation loss\n",
    "            best_models[(MAX_DEPTH, MIN_FREQ)] = (hyperparameter, valid_loss, model_state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3XK2a4rZidZ"
   },
   "outputs": [],
   "source": [
    "# Assuming results variable is already populated with the training results\n",
    "results_folder = 'saved_models'\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)\n",
    "\n",
    "# OPTION 1: SAVE ONLY BEST MODEL FOR EVERY COMBINATION OF GLOBAL PARAMS:\n",
    "\n",
    "for (MAX_DEPTH, MIN_FREQ), (hyperparameter, valid_loss, model_state_dict) in best_models.items():\n",
    "    embedding_dim, num_layers, dropout_rate = hyperparameter\n",
    "    # Define the filename based on the global parameter combination\n",
    "    filename = f'model_MD{MAX_DEPTH}_MF{MIN_FREQ}_es{embedding_dim}_nl{num_layers}_dr{dropout_rate}_loss{valid_loss:.6f}.pt'\n",
    "    filepath = os.path.join(results_folder, filename)\n",
    "    torch.save(model_state_dict, filepath)\n",
    "\n",
    "# # OPTION 2: SAVE EVERY MODEL\n",
    "# for result in results:\n",
    "#     MAX_DEPTH, MIN_FREQ, hyperparameter, valid_loss, model_state_dict = result\n",
    "#     embedding_dim, num_layers, dropout_rate = hyperparameter\n",
    "\n",
    "#     # Save the model with a filename indicating its parameters\n",
    "#     filename = f'model_MD{MAX_DEPTH}_MF{MIN_FREQ}_es{embedding_dim}_nl{num_layers}_dr{dropout_rate}_loss{valid_loss:.6f}.pt'\n",
    "#     filepath = os.path.join(results_folder, filename)\n",
    "#      torch.save(model_state_dict, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, MAX_DEPTH, model, custom_tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = custom_tokenizer(prompt, MAX_DEPTH)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(5):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, hidden = model(src, hidden)\n",
    "            # Directly modify the prediction vector to set EOS probability to 0\n",
    "            \n",
    "            eos_index = vocab['<eos>'] # As per your vocab setup\n",
    "            prediction[:, -1, eos_index] = -float('inf')\n",
    "\n",
    "            sos_index = vocab['<sos>'] # As per your vocab setup\n",
    "            prediction[:, -1, sos_index] = -float('inf')\n",
    "\n",
    "            probs = torch.softmax(prediction[:, -1], dim=-1)\n",
    "\n",
    "            # Get the top 5 probabilities and their indices\n",
    "            token_prob_pairs = [(vocab.get_itos()[index.item()], prob.item()) for index, prob in zip(torch.arange(probs.size(0)), probs.squeeze())]\n",
    "\n",
    "            # Return all tokens and their probabilities as a list of tuples\n",
    "            return token_prob_pairs\n",
    "\n",
    "\n",
    "            # Note: The original loop structure that appends to indices and continues generating\n",
    "            # has been simplified for this example. You might need to adjust this part based on your specific requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_folder='saved_models'\n",
    "for file in os.listdir(models_folder):\n",
    "    model_state_dict=torch.load(f'saved_models/{file}',map_location=torch.device(device))\n",
    "    \n",
    "    # load hyperparameters from the filename\n",
    "    model_name_without_extension = file.split('.pt')[0]\n",
    "\n",
    "    # Split the model name by underscore to get the components\n",
    "    components = model_name_without_extension.split('_')\n",
    "\n",
    "    # Extract the hyperparameters\n",
    "    model_prefix = components[0] # This is usually the model type or identifier\n",
    "    MAX_DEPTH = int(components[1][2:]) # Extract the number after 'MD'\n",
    "    MIN_FREQ = int(components[2][2:]) # Extract the number after 'MF'\n",
    "    embedding_size = int(components[3][2:]) # Extract the number after 'es'\n",
    "    num_layers = int(components[4][2:]) # Extract the number after 'nl'\n",
    "    dropout_rate = float(components[5][2:]) # Extract the number after 'dr'\n",
    "    loss = float(components[6][4:])\n",
    "    vocab= create_vocabulary(MIN_FREQ,MAX_DEPTH)\n",
    "    print(f'MODEL: {file}, MAX_DEPTH: {MAX_DEPTH}, MIN_FREQ: {MIN_FREQ}, embedding_size: {embedding_size}, num_layers: {num_layers}, dropout_rate: {dropout_rate}, loss: {loss}')\n",
    "    print(f'vocab_size = {len(vocab)}')\n",
    "    vocab_size = len(vocab)\n",
    "    tie_weights = True\n",
    "\n",
    "    model = LSTM(vocab_size, embedding_size, embedding_size, num_layers, dropout_rate, tie_weights).to(device)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    model.eval()\n",
    "    prompts=['<sos>']\n",
    "    # Softmax, put eos to 0% and ask again for probabilities\n",
    "    top_tokens, top_probs = generate(prompts[0],MAX_DEPTH,model,custom_tokenizer,vocab,device,0)\n",
    "    embedded_tokens_probs = [(top_tokens[i], f'{top_probs[0][i]*100:.2f}%') for i in range(5)]\n",
    "    print(f'DEPTH 1, starting token {prompts[0]}, predictions (tokens,probs): {embedded_tokens_probs}')\n",
    "\n",
    "    for tok, _ in embedded_tokens_probs:\n",
    "        url='<sos>/'+tok\n",
    "        print(\"Depth 2, prompt:\",url)\n",
    "        top_tokens_loc, top_probs_loc = generate(url,30,model,custom_tokenizer,vocab,device,0)\n",
    "        embedded_tokens_probs_loc = [(top_tokens_loc[i], f'{top_probs_loc[0][i]*100:.2f}%') for i in range(5)]\n",
    "        print(f'predictions (tokens,probs): {embedded_tokens_probs_loc}')\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
